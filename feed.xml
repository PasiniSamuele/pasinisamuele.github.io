<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://pasinisamuele.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pasinisamuele.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-03T12:29:12+00:00</updated><id>https://pasinisamuele.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Samuele Pasini. </subtitle><entry><title type="html">Adversarial Machine Learning</title><link href="https://pasinisamuele.github.io/blog/2024/NIST_paper/" rel="alternate" type="text/html" title="Adversarial Machine Learning"/><published>2024-01-15T22:45:19+00:00</published><updated>2024-01-15T22:45:19+00:00</updated><id>https://pasinisamuele.github.io/blog/2024/NIST_paper</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2024/NIST_paper/"><![CDATA[<p>Few days ago, I found an extremely <a href="https://www.nist.gov/publications/adversarial-machine-learning-taxonomy-and-terminology-attacks-and-mitigations">interesting work </a> published by NIST, regarding Adversarial Machine Learning <d-cite key="vassilev2024adversarial"></d-cite>. The authors explore several concepts and they define a taxonomy that I have found extremely useful to approach the research in the field of Adversarial Machine Learning.</p> <p>After exploring the most relevant attacks and mitigations, the authors propose open challenges. This work will be likely relevant in my future research, and I suggest to read it to anyone interested in the subject.</p>]]></content><author><name></name></author><category term="papers"/><category term="university"/><category term="llm"/><category term="deep-learning"/><summary type="html"><![CDATA[A Taxonomy and Terminology of Attacks and Mitigations]]></summary></entry><entry><title type="html">Back to University</title><link href="https://pasinisamuele.github.io/blog/2023/back_to_university/" rel="alternate" type="text/html" title="Back to University"/><published>2023-12-28T21:14:45+00:00</published><updated>2023-12-28T21:14:45+00:00</updated><id>https://pasinisamuele.github.io/blog/2023/back_to_university</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2023/back_to_university/"><![CDATA[<p>After long reflection, I decided to leave MindEarth after one year and to go back to the University to start a PhD.</p> <p>Despite being very happy with my job from both professional and personal point-of-view, I realized that my decision to interrupt my research career was hurried and mainly dictated by post-thesis stress.</p> <p>I felt to have a lot of energy to dedicate to a research activity and I realized that a PhD opportunity was quite unique. My reseach group is lead by Professor Paolo Tonella and it is located in Università della Svizzera Italiana (USI), in Lugano. My research will be related to Deep Learning and LLMs, with a specific focus on Adversarial/Poisoning attacks and the usage of LLMs for code generation.</p> <p>I think that the year spent in MindEarth will be extremely useful for my PhD and for my career and, at the same time, I realized that there is nothing wrong to retrace your steps and to make a different choice, if you think that this is the best for you! I hope that this experience will make the difference in my future!</p>]]></content><author><name></name></author><category term="experience"/><category term="university"/><category term="llm"/><summary type="html"><![CDATA[The beginning of my PhD journey]]></summary></entry><entry><title type="html">My Experience in a Startup</title><link href="https://pasinisamuele.github.io/blog/2023/startup_experience/" rel="alternate" type="text/html" title="My Experience in a Startup"/><published>2023-10-18T19:45:25+00:00</published><updated>2023-10-18T19:45:25+00:00</updated><id>https://pasinisamuele.github.io/blog/2023/startup_experience</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2023/startup_experience/"><![CDATA[<p>In the last few months before my graduation, I had several interviews and I have received many offerts. I also applied to a Ph.D. position in Politecnico di Milano.</p> <p>I was admitted to the position but, after a deep thought, I decided to refuse and to search for opportunities outside of the University. The decision was made because I thought that I had no more energy to devote to research, and also because I wanted to get out of the comfort zone of the university. My final choice was Mindearth, a Swiss company which operates with AI in several B2B and no-profit areas, combining Street-level imagery, Remote Sensing Images and mobility data.</p> <p>This was an amazing choice, in a small group such as in a startup I had the chance to “feel” the entire business structure of the company and all the pressure related to this.</p> <p>From a professional point-of-view, the reality of a startup forces you to solve problems out of your knowledge and that teaches very important soft skills. After a STEM degree, big companies can seem more comfortable, but if your goal is to increase your skill and if you are ready to feel the pressure, a startup is definitely a winning choice!</p>]]></content><author><name></name></author><category term="experience"/><category term="work"/><category term="computer-vision"/><summary type="html"><![CDATA[My first work experience and my suggestions]]></summary></entry><entry><title type="html">My Master Thesis in PoliMI</title><link href="https://pasinisamuele.github.io/blog/2022/master_thesis/" rel="alternate" type="text/html" title="My Master Thesis in PoliMI"/><published>2022-12-15T16:40:16+00:00</published><updated>2022-12-15T16:40:16+00:00</updated><id>https://pasinisamuele.github.io/blog/2022/master_thesis</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2022/master_thesis/"><![CDATA[<p>Last month I completed my Master Thesis in Politecnico di Milano, and I will graduate in Computer Science and Engineering this month. The thesis has occupied most of my time in the last few months, and it was a great satisfaction for me to see the results of this work.</p> <p>In Politecnico di Milano and in other universities there is also the chance of doing a short thesis, and this will certainly save you many hours of work and a lot of stress.</p> <p>However, my suggestion is to do NOT do it.</p> <p>I’ve had several interviews in the last few weeks, and, since I do not have any work experience, my Thesis was the main topic discussed and the greatest source of interest by the companies I have been in contact with. A short and less structured Thesis, such as a Portfolio, it would have denied me a big chance to make a positive impression.</p> <p>At the same time, it is relevant to choose a topic that is really found interesting, otherwise the stress and work load will become unmanageable.</p> <p>Let’s move to the content of my thesis.</p> <p>Following several Master courses, I have been fascinated by how the most recent advances in AI technology could have a huge impact on large amount of business and daily activities, being at the same time able to improve the quality of life in several manners. This is why I have done my Thesis in the context of Illegal Landfills Detection, in the hope that my work can make a positive contribution to the environment. The experience in studying and using several Computer Vision techniques such as Weakly Supervised Object Detection and Self-Supervised Learning was really helfpul to improve myself as a professional.</p> <p>I also had the chance to the Thesis with a fellow student, this was really interesting because we had the chance to support and improve each other, if there is the chance my suggestion is to do the same.</p> <p>The last relevant point is the choice of the Thesis Supervisor. It is crucial to have an supervisor available and willing to help, otherwise it is difficult to overcome the obstacles encountered during work. In my case, Professor. Piero Fraternali was a great choice, alongside his research group he led our work and he demonstrated that our work can touch on current and most important issues such as the environmental one, to which I am very attached, and that the research in this field can contribute to the development of solutions for problems that now more than ever require a rapid response.</p> <p>Thanks to his experience, we also had the chance to publish a Survey related to our Thesis work in MDPI - Remote Sensing, and I am very satisfied with this result.</p> <p>I think I’ve touched on all the relevant points of my thesis, good luck with yours!</p>]]></content><author><name></name></author><category term="experience"/><category term="university"/><category term="computer-vision"/><summary type="html"><![CDATA[My experience and my suggestions]]></summary></entry><entry><title type="html">Cycle and ECycle GAN</title><link href="https://pasinisamuele.github.io/blog/2022/cycle/" rel="alternate" type="text/html" title="Cycle and ECycle GAN"/><published>2022-11-27T18:32:15+00:00</published><updated>2022-11-27T18:32:15+00:00</updated><id>https://pasinisamuele.github.io/blog/2022/cycle</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2022/cycle/"><![CDATA[<p>Generative adversarial network (GAN) is a class of machine learning frameworks designed by Goodfellow et al. <d-cite key="GAN"></d-cite>. Two neural networks compete with each other. In particular, a discriminator and a generator are trained at the same time, competing in a sort of two players’ adversarial game. The Generator receives an input that belongs to a certain data distribution and produces a new sample that should belong to a target distribution. On the other hand, the Discriminator is fed with a sample that may or may not be produced by the generator and outputs the probability of the sample being real (not generated) or fake (generated).</p> <p>The goal of the generator is to fool the discriminator, producing realistic images belonging to the target domain, while the goal of the discriminator is to find a way to distinguish between real and fake samples.</p> <p>Traditionally, the input of the generator is sampled from a known distribution and it is referred to as noise. In case the input of the generator is an image of a specific (source) domain, that should be transformed into an image of a different (target) domain, the addressed task is <b>Image-to-Image translation</b> <d-cite key="im2im"></d-cite>.</p> <p>In particular, the aim is to learn the mapping between an input image and an output image, by using a training set of aligned image pairs. This could be useful for instance to perform style transfer, season transfer, and photo enhancement.</p> <p>Recently, different works have been developed to solve this task. For instance, Isola et al. <d-cite key="pix2pix"></d-cite> proposed Pix2Pix, a conditional adversarial network that learns the mapping from the input images to output images. However, training this network requires paired input-output images.</p> <p>Therefore, Zhu et al. <d-cite key="Zhu2017"></d-cite> proposed CycleGAN, a new architecture that learns to translate an image from a source domain \(X\) to a target domain \(Y\) in the absence of paired samples.</p> <p>CycleGAN makes use of two couples of GAN where each couple can transform images from one domain to the other. The goal is to learn a mapping \(G : X \rightarrow Y\) such that the distribution of generated images \(G(X)\) is indistinguishable from the distribution of images belonging to domain \(Y\). At the same time, this should hold also for the other domain. However, there are countless mappings between the two domains. For this reason, the authors introduced the cycle consistency loss to enforce that \(F(G(X)) \approx X\) and \(G(F(Y)) \approx Y\). This means that given an image \(x\) belonging to domain \(X\), that is transformed by \(G\) into an image \(\tilde{x}\) belonging to domain \(Y\), when \(\tilde{x}\) is transformed by \(F\) the output should be \(x\).</p> <p>An additional constraint is added since the authors show that it can provide better quality solutions. This constraint is called identity constraint and it enforces that \(F(X) \approx X\) and \(G(Y) \approx Y\). This means that the architecture should not modify an image if it already belongs to the target domain. To accomplish this task, the loss function is composed of multiple terms. First of all, as usual, the GANs are trained using an <b>adversarial loss</b> <d-cite key="GAN"></d-cite> . However, the original loss is replaced with a least square loss <d-cite key="LSGAN"></d-cite> since it ensures more stable training and higher quality results. Thus, the objective function to train the generator \(G : X \rightarrow Y\) and its corresponding discriminator \(D_{Y}\) is:</p> \[\displaylines{ \mathcal{L}_{\text{LSGAN}}[G,\ D_{Y}](X,\ Y)=\mathbb{E}_{y\sim p_{\text{data}}(Y)}[(D_{Y}(y)-1)^{2}] \\ +\mathbb{E}_{x\sim p_{\text{data}}(X)}[D_{Y}(G(x))^{2}] }\] <p>Another important term is the <b>cycle consistency loss</b> used to ensure both forward cycle-consistency, i.e., \(x \rightarrow G(x) \rightarrow F(G(x)) \approx x\), and backward cycle-consistency, i.e., \(y \rightarrow F(y) \rightarrow G(F(y)) \approx y\). This is done in a pixel-wise manner and is expressed as:</p> \[\label{eq:pixel-cyc} \displaylines{ \mathcal{L}_{pixel-cyc}[G, F](X,\ Y)=\mathbb{E}_{x\sim P_{\text{data}}(X)}[\lVert F(G(x)) - x \rVert_1 ] + \\ + \mathbb{E}_{y\sim P_{\text{data}}(Y)}[\lVert G(F(y)) - y \rVert_1 ] }\] <p>Finally, the last term to be considered is the <b>identity loss</b> used to ensure the identity constraints for both the \(X\) and \(Y\) domains and expressed via the following pixel-wise computation:</p> \[\label{eq:pixel-idt} \displaylines{ \mathcal{L}_{pixel-idt}[G, F](X,\ Y) =\mathbb{E}_{x\sim P_{\text{data}}(X)}[\lVert F(x) - x \rVert_1 ] \\ +\mathbb{E}_{y\sim P_{\text{data}}(Y)}[\lVert G(y) - y \rVert_1 ] }\] <p>Thus, the overall loss function is given by a weighted sum of these three terms:</p> \[\label{eq:overall} \displaylines{ \mathcal{L}[G, F, D_{X}, D_{Y}](X,\ Y) = \mathcal{L}_{\text{LSGAN}}[G,\ D_{Y}](X,\ Y) \\ +\mathcal{L}_{\text{LSGAN}}[F,\ D_{X}](Y,\ X) \\ +\lambda_{cyc} \mathcal{L}_{pixel-cyc}[G, F](X,\ Y) \\ +\lambda_{idt} \mathcal{L}_{pixel-idt}[G, F](X,\ Y) }\] <p>Even though CycleGANs can produce quite impressive results, for the task of 2-domain translation, the details about texture and style are often accompanied by unpleasant artifacts as reported in <d-cite key="Zhang2020"></d-cite>. Thus, to improve the effectiveness of domain translation, and obtain more realistic images, Zhang et al. <d-cite key="Zhang2020"></d-cite> proposed to modify the architecture, leading to the definition of Enhanced CycleGANs (<b>ECycleGANs</b>).</p> <p>One of the improvements suggested in the paper is to avoid relying only on pixel-wise losses to ensure cycle consistency since this could result in perceptually unsatisfying solutions with overly smooth textures. Thus, a loss function that takes into consideration the perceptual similarity is employed.</p> <p>This perceptual loss function includes a term named feature loss that is the euclidean distance between the high-level abstract feature representation of a cycle reconstructed image \(F(G(x))\) and the original image \(x\). The feature representations are extracted using a pre-trained 19 layer VGG network <d-cite key="vgg"></d-cite>. \(\phi_{i,j}\) identifies the feature map obtained by the \(j-th\) convolution (after activation) before the \(i-th\) max-pooling layer. This term is defined as:</p> \[\label{eq:ecycle_feature} \displaylines{ \mathcal{L}_{feature-cyc}[G, F](X,\ Y) \\ = \mathbb{E}_{i,j,x\sim P_{\text{data}}(X)}&amp;[\phi_{i,j}(F(G(x))) - \phi_{i,j}(x) ] \\ +\mathbb{E}_{i,j,y\sim P_{\text{data}}(Y)}&amp;[\phi_{i,j}(G(F(y))) - \phi_{i,j}(y) ] }\] <p>The feature loss and the pixel-wise loss are combined into the perceptual loss that is used as cycle consistency loss, where \(\alpha\) and \(\beta\) are the coefficients to balance these loss terms.</p> \[\label{eq:ecycle_perceptual} \displaylines{ \mathcal{L}_{perc-cyc}[G, F](X,\ Y) &amp;= \alpha(\mathcal{L}_{feature-cyc}[G, F](X,\ Y) \\ &amp;+ \beta(\mathcal{L}_{pixel-cyc}[G, F](X,\ Y)) }\] <p>To further improve the quality of the images produced by CycleGAN, a major adjustment to the structure of the generator is conducted. More specifically, the original basic residual block used inside the generator architecture is replaced with a Residual Dense Normalization Block (RDNB), which consists of a multi-level residual network, dense connection <d-cite key="densenet"></d-cite>, and instance normalization layers <d-cite key="instanceNorm"></d-cite>. This modification is performed following the fact that more layers and connections can optimize the performance of neural networks. However, to prevent instability in training a very deep network, a scaling factor named <b>residual scaling</b> is used to scale down the residuals. Thus, the perceptual loss encourages the translated images to be more realistic, while the introduction of RDNB blocks allows to generate high-quality images.</p> <p>I proposed an interesting work regarding Face Expression Data Augmentation considering both Cycle and ECycleGAN. You can find more detail about this projects <a href="https://pasinisamuele.github.io/projects/face_da/">at this link</a>.</p>]]></content><author><name></name></author><category term="papers"/><category term="university"/><category term="computer-vision"/><summary type="html"><![CDATA[Generative Adversarial Networks for Image-to-Image translation]]></summary></entry><entry><title type="html">Image Mosaicing</title><link href="https://pasinisamuele.github.io/blog/2022/mosaicing/" rel="alternate" type="text/html" title="Image Mosaicing"/><published>2022-10-18T16:24:16+00:00</published><updated>2022-10-18T16:24:16+00:00</updated><id>https://pasinisamuele.github.io/blog/2022/mosaicing</id><content type="html" xml:base="https://pasinisamuele.github.io/blog/2022/mosaicing/"><![CDATA[<p>Image mosaicing <d-cite key="GHOSH20161"></d-cite> is an effective means of constructing a single seamless image by aligning multiple partially overlapped images. In Computer Vision (CV), many applications, such as super-resolution imaging and medical imaging, require image mosaicing <d-cite key="Szeliski2006"></d-cite> require image mosaicing. This technique can also be used in panoramic stitching, allowing the creation of wide-angle images (without using fish-eye lenses) overcoming the difficulties in taking a photo with a very large field of view (FOV). Thus, image stitching algorithms have been used for decades to create the high-resolution photo-mosaics used to produce digital maps and satellite photos. Ideally, the resulting stitched image should be as natural as a real photo that covers the entire scene.</p> <p>However, while creating a mosaic from only two overlapping images is a relatively easy task and standard techniques can provide very good results, aligning multiple images is much more difficult, particularly if some input images do not overlap.</p> <p>More in depth, image mosaicing could be regarded as a special case of scene reconstruction where the images are related by planar homography only. This is a reasonable assumption if the images exhibit no parallax effects, <i>i.e.</i>, when the scene is approximately planar or the camera purely rotates about its optical centre.</p> <p>In general, this procedure can be divided into <b>image alignment</b> and <b>image compositing</b> steps. The goal of image alignment is to align the images into a common coordinate system (most of our work is related to this step). The goal of image compositing is to overlay the aligned images on a larger canvas by merging pixel values of the overlapping portions and retaining pixels where no overlap occurs. It is usually performed in two steps: colour correction and blending. Colour correction is needed since neighbouring images can present different colours due to factors such as the exposure level and differences in the lighting condition.</p> <p>As stated in <d-cite key="Santellani2018"></d-cite> the result of image mosaicing should not be confused with orthophotos. In fact, the former allows the visualization of a wide area on a single image under perspective projection, whereas the latter considers orthographic projections. For this reason, image mosaicing does not need any prior Structure-from-Motion or dense matching phases, that are instead required to generate orthophotos.</p> <p>Several methods for automatic image mosaicing are present in the literature, presenting a complete pipeline for the final mosaic generation <d-cite key="698630"></d-cite><d-cite key="1315099"></d-cite><d-cite key="Brown2007"></d-cite><d-cite key="doi:10.1080/02533839.2005.9670998"></d-cite> or focusing on one of the previously cited steps <d-cite key="Schroeder2011"></d-cite><d-cite key="LI20161"></d-cite><d-cite key="5995658"></d-cite>.</p> <p>The main focus of this post is related to the image alignement. The image alignment step refers to the alignment of the images into a common coordinate system using the computed geometric transformations. Existing algorithms <d-cite key="Szeliski2006"></d-cite> for this task are broadly categorised based on the information they extrapolate from the image.</p> <p><i>Direct methods</i> exploit the entire image data, thus providing very accurate registration but requiring at the same time a close initialization. They either compute the similarity based on image intensity values or based on the quantity of information (mutual information) shared between two images.</p> <p>In contrast, <i>feature-based algorithms</i> rely on the computation of transformations using a sparse set of low-level features and can be computationally less expensive. Commonly used low-level features (<i>e.g.</i>, edges, corners, pixels, colors, histograms) can be extracted exploiting a variety of approaches <d-cite key="Lowe2004"> </d-cite><d-cite key="10.1007/11744023_32"> </d-cite><d-cite key="10.1007/11744023_34"></d-cite><d-cite key="6885761"></d-cite>. In particular, <b>Brown et al.</b> <d-cite key="Brown2007"></d-cite> proved that formulating stitching as a multi-image matching problem and using invariant local features to find matches between the images, allows building a method insensitive to ordering, orientation, scale and illumination of the input images.</p> <p>Image alignment and colour correction can be both solved using graph synchronization techniques. The synchronization problem can be defined as follows:</p> <blockquote><p>Given a graph where nodes are characterized by an unknown state, and edges measure the ratio (or difference) between the states of the connected nodes, try to infer the unknown states from the pairwise measures.</p><cite><a class="citation" href="#"></a></cite></blockquote> <p>More precisely, states are represented by the elements of a specific group (this is why the problem is referred to as group synchronization). Recently, the synchronization problem has been extensively investigated in the Computer Vision community <d-cite key="Schroeder2011"></d-cite><d-cite key="Arrigoni2020"></d-cite><d-cite key="DalCin2021"></d-cite>. <b>Schroeder et al.</b> <d-cite key="Schroeder2011"></d-cite> proposed four closed-form solutions to the synchronization problem for the specific task of image alignment. In this specific scenario, the global homographies represent the unknown states of a graph where the edges are pairwise homographies. For this reason, states belong to the <i>SL(3)</i> group (Special Linear group, <i>i.e.</i>, set of 3x3 matrices with unit determinant).</p> <p><b>Arrigoni et al.</b> <d-cite key="Arrigoni2020"></d-cite> review several methods based on synchronization where the groups have a matrix representation, that allow closed-form solutions.</p> <p><b>Dal Cin et al.</b> <d-cite key="DalCin2021"></d-cite> proposed an algorithm (MULTISYNC) for solving the synchronization problem in the case of multi-graphs (graphs with multiple edges connecting the same pair of nodes) based on an expansion algorithm coupled with a constrained spectral solution to deal with replicated nodes. Our work moves in the direction of <d-cite key="DalCin2021"></d-cite> trying to apply multi-graph synchronization in the image alignment scenario as considered in <d-cite key="Schroeder2011"></d-cite>.</p> <p>As explained in <d-cite key="DalCin2021"></d-cite>, the basic solution to multi-graph synchronization is <i>edge averaging</i>, <i>i.e</i>, converting a multi-graph into a simple-graph by averaging the measurements of the edges having the same source and destination nodes. However, edge-averaging is not well defined for all the groups. For instance, while it is possible to average rotations <d-cite key="Hartley2012RotationA"></d-cite>, there is not a theoretically sustained averaging for homographies. Thus, we study the results provided by edge averaging in the case of homographies and compare them with multi-graph synchronization. The same multi-graph framework <d-cite key="DalCin2021"></d-cite> can be applied to partition classical synchronization tasks, achieving a good trade-off between accuracy and complexity. The whole procedure can be seen as composed of three main steps:</p> <ul> <li> <b>Graph Building</b>: in this phase, the graph representation of the pairwise homographies, able to align one image to another one, is built.</li> <li> <b>Image Alignment</b>: in this phase, multi-graph synchronization is applied to the previously constructed graph. Thanks to the synchronization algorithm, the unknown states representing global homographies are inferred. Differently from pariwise hommographies, global homographies are able to align the images to the common coordinate system.</li> <li> <b>Image Stitching</b>: in this phase, the stitched image is created. Each image is transformed in the reference frame by exploiting the estimated global homographies and fused with the others.</li> </ul> <p>I proposed applied MULTISYNC to solve partitioned synchronization problems estimating global homographies for image mosaicing. You can find more detail about this projects <a href="https://pasinisamuele.github.io/projects/image_mosaicing/">at this link</a>.</p>]]></content><author><name></name></author><category term="papers"/><category term="university"/><category term="computer-vision"/><summary type="html"><![CDATA[Image alignement for mosaicing images, with focus on synchronization algorithms]]></summary></entry></feed>