<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Time Series Prediction | Samuele Pasini </title> <meta name="author" content="Samuele Pasini"> <meta name="description" content="Analysis and Predition of Time Series for Polimi Competition"> <meta name="keywords" content="deep-learning, computer-vision, nlp, llm"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_1.svg?9a3b97ebf28cb15c27a88cdaab2d00d4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pasinisamuele.github.io/projects/time_series/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Time Series Prediction",
            "description": "Analysis and Predition of Time Series for Polimi Competition",
            "published": "August 29, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Samuele </span> Pasini </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/moments/">Moments </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Time Series Prediction</h1> <p>Analysis and Predition of Time Series for Polimi Competition</p> </d-title> <d-article> <p>I worked on this project for a competition during the course of Artificial Neural Network and Deep Learning in Politecnico di Milano. I worked with Dr. Francesco Azzoni and Dr. Corrado Fasana.</p> <h2>Analysis of the Data</h2> <p>The data is a multivariate time series with 7 features, with different ranges and without evident outliers. Before proceeding with modelling we normalized the data with MIN-MAX normalization and analyzed it using time-series decomposition. In doing this, we noticed high correlation between some pairs of features (Crunchiness - Hype root; Loudness - Wonder level). By looking at the autocorrelation and partial-autocorrelation plots, it seems that the variables can be modelled using an AR model given that the PACF has a hard cut-off at a certain lag. Moreover, the ACF plot shows a high correlation between the value of the time series at time $t$ with that at time \(t-96\). This highlights the presence of a seasonality that can be easily visualized plotting the time series.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/time_series/tsa-480.webp 480w,/assets/img/time_series/tsa-800.webp 800w,/assets/img/time_series/tsa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/time_series/tsa.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="AerialWaste" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>By plotting the series it is possible to notice that there are intervals of time in which features show a constant value (maybe data-glitches). For this reason, we created a copy of the training set, removing these portions paying attention to keep seasonal properties unchanged. To do this, we removed slices of 96 elements at the time.</p> <h2>Data preparation</h2> <p>To perform a proper splitting, we had to define 3 hyper-parameters: window, stride and telescope. We split the dataset in training and validation sets that have \(window\) common samples. This is done to perform the evaluation also on the first \(telescope\) samples after the training set, that otherwise would only be considered as part of the \(window\) and never used to evaluate the metrics.</p> \[\label{eq:split} (set\_size - window - telescope) \; \% \; stride == 0\] <p>The size of the sets is trimmed in order to ensure that the equation holds. This is done to avoid using padding when building the sequences that constitute a batch. Thus part of the oldest samples are removed during this procedure. We normalized both training and validation sets with MIN-MAX normalization and we built sequences starting from them.</p> <h2>Scheduled Sampling</h2> <p>Since we use a value for the \(telescope\) that is smaller than the number of samples that we need to forecast, we need to use an auto-regressive procedure. To improve the quality of the predictions during the regressive steps we applied a scheduled sampling procedure: the idea is to initially let the model learn on the original sequences for a certain number of epochs, then start substituting (with an increasing probability) some portions of the sequences with values predicted by the model. In this way the model learns how to perform predictions based on its own past predictions. This should avoid the risk of having performance drop during forecasting after the first autoregressive step. However, in the end, we noticed that models without scheduled sampling outperformed the same models with scheduled sampling.</p> <h2>Hyper-parameters tuning</h2> <p>At the beginning, the different models were tried with standard hyper-parameters, to get a baseline over the possible performances. Then, in order to tune them in a better way, a grid-search hold-out cross-validation was performed. Anyway, working in the local environment, we faced some problems due to the limited amount of resources, partially tackled using packages like Ray. For this reason, we did not perform cross-validation for the more complex models used later on.</p> <h2>Regularization</h2> <p>In order to avoid overfitting, we combined different regularization techniques. In particular, we used early stopping during the training procedure and we built most of the models with the inclusion of some dropout layers.</p> <h2>Models</h2> <p>In order to address the task, we built several models with different types of architectures and different parameters. As already said, the parameters used in the first models were those found using cross-validation. However, for complex models, we performed some trials and errors to find satisfiable parameters.</p> <h2>Hierarchical LSTM</h2> <p>Initially, we started by building simple models composed of up to 2 or 3 stacked LSTM cells with and without convolutional layers. This was not expected to give outstanding performances given the simplicity of the model but the results could then be used as a baseline for further models. In particular, we also tried to use the same models with GRU cells and using bidirectional LSTMs. In the end, the resulting predictions appeared quite smooth w.r.t. the real time-series appearance and the performance on the hidden test was not lower than 4.2 for what concern the RMSE.</p> <h2>Vanilla Transformer</h2> <p>After the different trials using methods that make use of recurrent connections, we decided to try to use a transformer to improve the performances using self-attention mechanism and positional encoding instead of RNNs. Transformers should be more efficient and accurate on long-range predictions, since they are able to process a longer input sequence and detect the most relevant features to produce the next output. The architecture that we initially used is the one shown in the paper “Attention Is All You Need”, composed by several encoder and decoder blocks and self-attention mechanism. The input of the decoder during training is composed by the last value (or the last few values) of the encoder input and then the true target values until we compose a sequence of length \(telescope\). A look-ahead mask must then be applied in the decoder attention layers to make sure that values at step \(t+i\) are not used to compute the attention scores of values at step \(t\). The performance was good during training, but pretty bad in prediction. This is probably caused by the Teacher Forcing learning method used as training procedure: the model relies too much on the decoder input and thus learns to produce as output this same input, because it is very similar to the true target (it is just shifted). During the prediction phase instead, it is not the true target that is fed to the decoder but the value predicted at the previous time-step(s), thus the learned strategy performs poorly. For this reason we tried to decrease the number of true values fed to the decoder, but the performance did not improve much.</p> <h2>Customized Transformer</h2> <p>Given the previous results, we decided to try to remove the decoder part completely and only keep the encoder blocks. For this reason, the general architecture that we used was composed of a set of self-attention layers (with multi-head attention) followed by a few dense layers in order to perform the final prediction. Given that no recurrent components are used, it is necessary to provide some further information that allows the model to take into account time and ordering of the values of the time series. This was addressed using time embedding (or positional encoding in the case of the Vanilla transformer). In particular, we used Time2Vec.</p> <p>According to the paper in which this embedding was presented, Time2Vec gives a vectorial representation of time with <b>periodicity</b>, <b>invariance to time rescaling</b> and <b>simplicity</b>. For a given scalar notion of time \(\tau\), Time2Vec of \(\tau\) , denoted as \(t2v(\tau)[i]\), is a vector of size \(k + 1\) defined as follows:</p> \[t2v(\tau)[i]= \begin{cases} \omega_{i}\tau + \phi_{i},&amp; \text{if} \; i = 0. \\ \mathcal{F}(\omega_{i}\tau + \phi_{i}),&amp; \text{if} \; 1 \leq i \leq k. \\ \end{cases}\] <p>where \(t2v(\tau)[i]\) is the \(i^{th}\) element of \(t2v(\tau)[i]\), \(\mathcal{F}\) is a periodic activation function, and \(\omega_{i}\)s and \(\phi_{i}\)s are learnable parameters.</p> <p>Thus, in the end, the time series is passed through the embedding layer before being fed to the transformer. One of the most important aspects to be considered is related to the dimension of the embedding representation provided by Time2Vec. Initially, we set it to 1 and tried to check whether the transformer was able to achieve promising performances. Of course, also the number of attention layers, the number of heads and other parameters like the dropout rate should be properly tuned. After trying the previous model, we realised that the results were already comparable to those provided by the hierarchical LSTM architectures. Thus, we proceeded by experimenting with different hyper-parameters values and with the use of scheduled sampling. In this way, we were able to reach better performances, the best providing a RMSE of around 3.50 on the hidden test set. During these experiments, we found that suitable Time2Vec dimensions were 2 or 5 and that the use of different window, stride an telescope could have a big impact on the performances too. Moreover, we also noticed that using ELU and SELU activation functions provided better results than a classic RELU. In the end, we also tried to use RMSProp optimizer instead of Adam but the latter showed almost always better results. Finally, another important aspect is the learning rate scheduler. In fact, the use of different learning rate schedulers impacted the stability of the learning process and the capability of converging faster.</p> <h2>Customized Transformer 2.0</h2> <p>In the previous models we always used dense layers after the flat vector of features extracted by the Transformer. In this architecture instead we took the output of the attention blocks, that has a shape of <b>(None, window, features)</b>, and sliced it along the dimension $1$ to have several blocks of shape <b>(None, period, features)</b> capturing the information of just one seasonality. Those blocks are first processed separately to extract the most useful features and to reduce the depth, feeding them to a small network composed by: a 1D convolutional layer, a flattening layer, a dense layer and a reshape layer resulting in a shape of <b>(None, period, target_features)</b>. Then they are concatenated along a new axis producing a 4D tensor of shape <b>(None, period, target_features, blocks)</b>. A series of 2D convolutional layers is used to extract the relevant patterns, reducing after each one the temporal dimension using a stride greater than 1. After them a <b>1x1</b> 2D convolutional layer is used to change the depth of the tensor in order to match the desired output size <b>(telescope x target_features)</b>, and then a GAP layer is added to combine together the dimensions 1 and 2 resulting in a tensor with shape <b>(None, 1, 1, telescope x target_features)</b>. The last operation is to reshape the features to achieve the desired output shape <b>(None, telescope, target_features)</b>. We tuned the hyper-parameters of this architecture and after some trials and errors we managed to achieve a better performance w.r.t. the previous models. In particular, we were able to reach a RMSE on the hidden test of 3.41 using the original dataset. Instead, when we moved to the use of the cleaned dataset, keeping the same model, the RMSE on the hidden test decreased to the value 3.33.</p> <h2>Model ensemble</h2> <p>As last try, we performed model ensemble using the best models constructed so far. Thus, the final prediction was computed as the average of the prediction provided by each model. The final performance was slightly better than before (3.32). In the end, we also tried to perform model ensemble using a fully convolutional meta-learner trained on top of the predictions provided by the other models.</p> <h2>Other useful information</h2> <p>We tried to train the models considering either the MSE or the MAE as loss function. In the end, the MAE seems to be a better choice. Finally, before submitting the models we always performed a full training of them using the whole dataset.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Samuele Pasini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>