<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Facial Expression DA | Samuele Pasini </title> <meta name="author" content="Samuele Pasini"> <meta name="description" content="A generative approach for Facial Expression Data Augmentation"> <meta name="keywords" content="deep-learning, computer-vision, nlp, llm"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_1.svg?9a3b97ebf28cb15c27a88cdaab2d00d4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pasinisamuele.github.io/projects/face_da/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Facial Expression DA",
            "description": "A generative approach for Facial Expression Data Augmentation",
            "published": "June 17, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">SamueleÂ </span> Pasini </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Facial Expression DA</h1> <p>A generative approach for Facial Expression Data Augmentation</p> </d-title> <d-article> <p>Image-to-Image translation aims to transfer images from a source domain to a target one while preserving the content representations. It can be applied to a wide range of applications, such as style transfer, season transfer, and photo enhancement.</p> <p>To accomplish this task several architectures have been proposed, including CycleGANs <d-cite key="Zhu2017"></d-cite> which are composed of a pair of GANs, and their improved version Enhanced CycleGAN (ECycleGAN) <d-cite key="Zhang2020"></d-cite>. I already analyzed these networks in <a href="https://pasinisamuele.github.io/blog/2022/cycle/"> this blog post </a> which I suggest you to read before continuing.</p> <p>In the context of Deep Learning, one major problem is the need for huge datasets to effectively train a deepmodel. However, the amount of available images can be limited and dependent on the specific class, leading to unbalanced datasets. To solve this problem, a large amount of different data augmentation techniques have been developed during the last years.</p> <p>I presented this project with Dr. Francesco Azzoni and Dr. Corrado Fasana to the Advanced Deep Learning Models and Methods exam in Politecnico di Milano.</p> <p>The work exploits the advancements in Image-to-Image translation to perform data augmentation of facial expression data.</p> <p>The effectiveness of the proposed method is assessed analysing the classification performance on the unbalanced FER2013 facial expression dataset <d-cite key="fer2013"></d-cite>.</p> <p>Before exploiting ECycleGANs to perform data augmentation, an attempt was made to reproduce the results obtained by Zhu et al. <d-cite key="Zhu2018"></d-cite> to have a model for comparison. The authors used a subset of the dataset to perform the experiments, but the sampling strategy is left unspecified. The result obtained using a random sampling strategy did not match those published in the paper. By analyzing the original dataset it is evident that a random sampling strategy led to the usage of low quality samples jeopardizing the result, and more sophisticated ways to select the dataset subset must be explored.</p> <p>The original FER2013 dataset <d-cite key="fer2013"></d-cite> has 7 classes, namely: <i>Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral </i>. The total number of samples is 35887, and some classes are more represented then others (<i>e.g.,</i> <i>Disgust</i> has 547 samples while <i>Happy</i> 8989). However, due to the previous observations, a filtered version of the dataset was used. The filtered dataset instead has 28941, keeping the same imbalanced nature of the classes (<i>e.g.,</i> <i>Disgust</i> samples: 421, <i>Happy</i> samples: 7967). 100 images per class are kept apart to test the multi-class classifier, while the remaining ones are used for training.</p> <p>FER2013 dataset <d-cite key="fer2013"></d-cite> is characterized by high intra-class diversity, high inter-class similarity, and the presence of mislabeled samples and samples belonging to other domains. Thus, the following techniques were employed to try to mitigate these problems and check whether the performances reported in <d-cite key="Zhu2018"></d-cite> could be replicated:</p> <ul> <li> <b>Gaussian likelihood</b>: the idea is to perform instance selection by removing those instances that lie in low-density regions of the data manifold. This is done by exploiting the method proposed in DeVries et al. <d-cite key="NEURIPS2020_99f6a934"></d-cite> which computes the likelihood of each image using a Gaussian model fit on feature embeddings produced by a pre-trained Inceptionv3 classifier <d-cite key="inceptionV3"></d-cite>. Then, only those samples with a likelihood greater than a certain threshold are kept. </li> <li> <b> Confidence Filtering </b>: the idea is that first a classifier is trained on the whole dataset. All the samples are then evaluated using the classifier and ranked according to the probability of belonging to their annotated class. Using as threshold a minimum confidence or a number of samples it is possible to obtain a filtered version of the dataset where most of the ambiguous samples are discarded. </li> </ul> <p>However, even though both these approaches seemed to correctly remove ambiguous samples, the performance reported in the paper <d-cite key="Zhu2018"></d-cite> could not be reached.</p> <p>Once the dataset has been filtered, the next step is the implementation and training of the EcycleGAN model. Starting from the existing implementation of CycleGAN provided by <d-cite key="Zhu2017"></d-cite> several adjustments are made to match the architecture described in <d-cite key="Zhang2020"></d-cite>:</p> <ul> <li> Instead of the original residual block <d-cite key="resnet"></d-cite> of the CycleGAN generator a deeper and more complex one is employed, called RDNB.</li> <li> The perceptual loss is implemented in place of the original pixel-wise loss. </li> <li> The Wasserstein GAN objective with gradient penalty (WGAN-GP) <d-cite key="wgangp"></d-cite> is implemented as an alternative to the LSGAN objective function <d-cite key="LSGAN"></d-cite> to stabilize the training. Both have been tested and compared in the experiments section.</li> </ul> <p>Given the fact that the number of samples of some classes is very limited, the discriminator of such classes tends to overfit. Several GAN-specific techniques have been proposed to mitigate this problem:</p> <ul> <li> <b>One-sided Label smoothing</b> <d-cite key="label_smoothing"></d-cite>: it is concerned with the addition of label noise. More precisely, the discriminator is trained on randomly flipped labels instead of real labels. This label noise is applied only to the samples of the less represented class when fed to the discriminator.</li> <li> <b>Instance Noise</b> <d-cite key="label_smoothing"></d-cite>: in this case, the discriminator sees the correct labels, but its input sample is noisy. This avoids the saturation of the discriminator objective, reducing overfitting. The added noise is Gaussian with zero mean and decaying standard deviation during training.</li> <li> <b>Alternate training</b>: when the training procedure is initiated (after a fixed number of epochs) the discriminator of the less represented class is not trained at every iteration. Since the number of samples is small the discriminator is prone to overfitting, thus it can discriminate between real and fake samples with very low uncertainty. Hence, training it less frequently than the generator gives the latter more time to learn.</li> </ul> <p>After the implementation of the proposed approaches, to compare the quality of the synthetic data generated using CycleGAN and especially ECycleGAN, several experiments were conducted using different parameters and finally compared. The final performance of the classifier on the original and augmented dataset are assessed using 3x10-fold cross-validation and averaging the results.</p> <p>For the first experiments, the employed CycleGAN architecture and parameters are the same as <d-cite key="Zhu2018"></d-cite>.</p> <p>The paper does not consider the identity loss, so different experiments were performed weighting the identity loss in different ways (\(\lambda_{idt} = [0.0; 0.3; 0.5]\)). For each configuration, after training, 100 <i>Disgust</i> images are generated starting from a fixed set of <i>Neutral</i> images and added to the original dataset to assess the impact on the classifier performance. Initially, the cycle consistency loss weight was set to \(\lambda_{cyc} = 10\) as in the paper<d-cite key="Zhu2018"></d-cite>. However, given that the generated images were extremely similar to the input ones, the \(\lambda_{cyc}\) was fixed to 1 for most of the following attempts, leading to a more significant impact on the image, while maintaining a convincing cycle consistency and a low reconstruction error. Finally, some generated samples are also manually selected to compare the different settings from a qualitative perspective.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/cycle_gan-480.webp 480w,/assets/img/cycle_gan/cycle_gan-800.webp 800w,/assets/img/cycle_gan/cycle_gan-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/cycle_gan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cycle gan" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The experiments of ECycleGAN started replicating the same base configuration reported in Zhang et al. <d-cite key="Zhang2020"></d-cite>., which makes use of WGAN-GP loss function <d-cite key="wgangp"></d-cite>. and RDNBs with 3 dense-blocks, each composed of 5 convolutional densely connected sub-blocks.</p> <p>Unfortunately due to the limited available resources neither training such a powerful network nor performing a complete hyper-parameters tuning is feasible. Thus, the number of dense blocks and sub-blocks is decreased to 2 and 3 respectively. Given the poor results and the divergence problems that arose using the proposed WGAN-GP, the following experiments were performed using LSGAN <d-cite key="LSGAN"></d-cite>. that resulted in a more stable training. The coefficient for the Cycle-loss \(\lambda_{cyc}\) is set to the same value used in the CycleGAN case, and the same holds for the \(\lambda_{idt}\) values. Regarding the perceptual loss, the balancing coefficients \(\alpha\) and \(\beta\) are both set to \(0.5\) after some trial and error, and the feature map considered for the feature-loss component is the one generated by the last convolutional layer of the VGG19 feature-extractor. Finally, the residual scaling of the RDNBs is tuned by choosing between the values \(\lambda_{idt} = [0.3; 0.5; 0.7]\), balancing the influence of the residual connection. As done for CycleGANs, generated samples are manually selected to compare the different settings from a qualitative perspective.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/ecycle_gan-480.webp 480w,/assets/img/cycle_gan/ecycle_gan-800.webp 800w,/assets/img/cycle_gan/ecycle_gan-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/ecycle_gan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ecycle gan" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Given that the discriminator of the less represented class tends to overfit in the previous models, the model providing the more promising results was selected and enhanced with the techniques previously to try to mitigate this problem. More specifically, Label-smoothing was applied with a label flip probability of \(1\%\). Instance-noise was implemented using a Gaussian noise starting with a standard deviation of \(0.1\) and \(0.05\) then linearly decaying, while in the case of Alternate-training the training ratio generator-discriminator was set to \(10:1\) or \(5:1\). As done for CycleGAN, for each trained model, 100 <i>Disgust</i> images are generated starting from the same fixed set of <i>Neutral</i> images for better comparison and added to the original dataset to assess the impact on the classifier performance. Then, the most promising model is used to generate also 200 and 500 images to check whether a bigger augmentation can further boost the classifier performance. Finally, a few experiments were performed to generate <i>Surprise</i> images from <i>Neutral</i> ones, to check the impact of the imbalance gap.</p> <p>The obtained results were evaluated both qualitatively and quantitatively. In particular, a qualitative evaluation of the images generated by each experiment was performed first. Then, a quantitative evaluation of multi-class classification was assessed using different metrics.</p> <ul> <li> Mean <i>Precision</i>. </li> <li> Per-Class <i>Precision</i>. </li> <li> Mean <i>Receiver Operating Characteristic - Area Under the Curve (ROC-AUC)</i>. </li> <li> Per-Class <i>ROC-AUC</i>. </li> </ul> <p>The idea is that the Precision can provide a first very intuitive indication of the classifierâs capabilities, while the AUC tells how much the model is capable of distinguishing between classes (in a one-vs-all setting).</p> <p>The main improvement from CycleGAN to ECycleGAN is related to the quality of the generated images.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/ecycle_vs_cycle-480.webp 480w,/assets/img/cycle_gan/ecycle_vs_cycle-800.webp 800w,/assets/img/cycle_gan/ecycle_vs_cycle-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/ecycle_vs_cycle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ecycle_vs_cycle" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Also, the ratio between the number of images of the two domains, and the consequent loss behavior, have a huge impact on the quality of the generated samples. The smaller the gap (<i>e.g., Neutral-Surprise</i> translation), the better the quality.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/ecycle_vs_cycle_surprise-480.webp 480w,/assets/img/cycle_gan/ecycle_vs_cycle_surprise-800.webp 800w,/assets/img/cycle_gan/ecycle_vs_cycle_surprise-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/ecycle_vs_cycle_surprise.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ecycle_vs_cycle_surprise" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The techniques previously proposed to tackle the overfit were adopted to force the loss behavior of the Neutral-Disgust experiment in which the samples gap is far larger, to be similar to the Neutral-Surprise one, wishing to increase the quality of the results. While the loss behavior was effectively changed as desired, the quality of the images does not show a convincing improvement.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/overfitting_mitigation-480.webp 480w,/assets/img/cycle_gan/overfitting_mitigation-800.webp 800w,/assets/img/cycle_gan/overfitting_mitigation-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/overfitting_mitigation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="overfitting_mitigation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The same classification setup is used for all the experiments, taking into consideration previously cited metrics for evaluation. The classification performance on the non-augmented filtered dataset is used as the baseline.</p> <p>Among the CycleGAN experiments there is a noticeable improvement in the <i>Disgust</i> precision w.r.t. the baseline, but at the same time the other classesâ precision decreases, resulting in a mean precision that is similar to the baseline one. This is because only the samples that are clearly disgusted are classified as such, reducing the false positives of class <i>Disgust</i>. The other less certain Disgust samples are assigned to other classes, increasing their false positives and reducing their precision. According to the AUC the quality of the augmentation depends on the value of \(\lambda_{idt}\) used: the higher the value the lower the AUC. Thus, it is possible to derive that synthetic samples that have features similar to real faces (more probable with higher \(\lambda_{idt}\)) are more easily misclassified since the stronger identity constraint does not allow to make them look disgusted. Thus, CycleGAN architecture is not powerful enough to allow better discrimination between classes, leading to a mean AUC which is similar to that of the baseline.</p> <p>Regarding the ECycleGAN the first experiments were performed to establish the best value of residual scaling (\(\alpha\)) and identity loss weight \(\lambda_{idt}\) hyper-parameters. When using \(\lambda_{idt} = 0.5\) and \(\alpha=0.5\) the best overall improvement is obtained for both metrics. Different values of these parameters cause input images to be modified too much or not enough. Using the best model, different techniques to avoid overfitting were employed, however even though there is an improvement concerning the generator loss convergence, no significant performance boost was observed. Increasing the number of augmented images to 200 or 500 decreases the performance due to the introduction of too many bad-quality samples. Finally, considering a more represented class such as <i>Surprise</i>, the ECycleGAN qualitative performance is surprising also w.r.t. CycleGAN. However, there is still not much improvement in the classifier performances since this class is already quite distinguishable from the others and thus, the introduction of some lower quality samples can even slightly decrease the performances w.r.t. the baseline.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/precision-480.webp 480w,/assets/img/cycle_gan/precision-800.webp 800w,/assets/img/cycle_gan/precision-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/precision.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="precision" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cycle_gan/auc-480.webp 480w,/assets/img/cycle_gan/auc-800.webp 800w,/assets/img/cycle_gan/auc-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cycle_gan/auc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="auc" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Our experiments show that ECycleGAN can be more effective than CycleGAN for Facial Expression Data Augmentation. Despite the quality of some generated samples is good, the influence of bad generated samples is too high to augment the dataset significantly. While introducing a low number of generated samples (around 100) leads to a limited performance boost, when introducing more samples (from 200 on), the major influence of the bad-quality ones results in a degradation of the classification performances since the intra-class diversity increases too much.</p> <p>However, the improvement w.r.t. the previously proposed CycleGAN model is present from both qualitative and quantitative viewpoints.</p> <p>Further experiments and studies could be conducted by adopting the complete ECycleGAN architecture and performing a finer hyper-parameter tuning if resources are available. Moreover, other filtering methods for the input dataset could be explored, as well as an instance selection algorithm to choose the best samples generated by ECycleGANs. Other techniques to avoid overfitting the discriminator could be considered. Finally, pretraining the VGG19 network used for the perceptual loss with a pretext task on human faces could help to extract more suitable features to be considered for the consistency losses, boosting the performance.</p> <p>You can find the implementation in the <a href="https://github.com/PasiniSamuele/ECycleGAN-face-emotion" rel="external nofollow noopener" target="_blank">linked repository</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2022-11-27-cycle.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Samuele Pasini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>