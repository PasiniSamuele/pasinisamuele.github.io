<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM Research Assistant | Samuele Pasini </title> <meta name="author" content="Samuele Pasini"> <meta name="description" content="How to build a Research assistant with LLM"> <meta name="keywords" content="deep-learning, computer-vision, nlp, llm"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon_1.svg?9a3b97ebf28cb15c27a88cdaab2d00d4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pasinisamuele.github.io/projects/LLM_research_assistant/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LLM Research Assistant",
            "description": "How to build a Research assistant with LLM",
            "published": "May 19, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Samuele </span> Pasini </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/moments/">Moments </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LLM Research Assistant</h1> <p>How to build a Research assistant with LLM</p> </d-title> <d-article> <p>In the last few years, LLMs were real game changers in a lot of domains. However, LLMs are just models trained to predict the next single token, and this is not enough to build complex application.</p> <p>The need of chaining the LLMs with traditional software stack to build larger applications gave rise to <b>Langchain</b>, one of the most fast-growing Open-Source projects of all time.</p> <p>The simple idea of Langchain is to allow the interaction between models and other software (APIs, databases, …) through a common interface.</p> <p>One of the most powerful technologies (simply integrable with Langchain) is <b>Retrieval Augmented Generation</b> <d-cite key="lewis2020retrieval"></d-cite>.</p> <p>RAG was proposed by Meta in NeurIPS 2020. They combined LLM (parametric memories) with information retrieval (non-parametric memories) to increase and update the knowledge without additional training. More specifically, they combine these components in a probabilistic model:</p> <ul> <li>The retriever provides latent documents conditioned on the input.</li> <li>The seq2seq model conditions on these latent documents together with the input to generate the output.</li> </ul> <p>A typical RAG application has two main components:</p> <ul> <li> <b>Indexing</b>: a pipeline for ingesting data from a source and indexing it. This usually happens offline.</li> <li> <b>Retrieval and generation</b>: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.</li> </ul> <p>The indexing is composed by several substeps in Langchain:</p> <ul> <li> <b>Load</b>: First we need to load our data. This is done with <b>DocumentLoaders</b>.</li> <li> <b>Split</b>: <b>Text splitters</b> break large <b>Documents</b> into smaller chunks. This is useful both for indexing data and for passing it into a model, since large chunks are harder to search over and won’t fit in a model’s finite context window.</li> <li> <b>Store</b>: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a <b>VectorStore</b> and <b>Embeddings</b> model.</li> </ul> <p>The problem faced in this project is to have a large bibliography with a huge number of papers, and they are the starting point to write a paper considering those topics. It is hard for an LLM to help because the knowledge required is really specific, otherwise the writing style will not be suitable for a scientific paper. LLMs suffers also of hallucination problems, so it is hard to understand if they are generating wrong sentences. The idea is to consider to integrate RAG over these papers to have a chatbot able to help.</p> <p>I started with the bibliography of my Master Thesis (40 papers) about WSOD in RSI. I used to the LLM to answer a question about PCIR, one of the most relevant networks in the literature. There is not additional experiment/question, since it was made just for assessment.</p> <p>I used the Langchain framework, and most of the tools in the indexing pipeline are the default ones:</p> <ul> <li>RecursiveTextSplitter as splitter</li> <li>Chroma as vectorstore</li> <li>OpenAIEmbeddings as embeddings</li> <li>I choosed Grobid as DocumentLoader</li> </ul> <p>Grobid is a machine learning library for extracting, parsing, and re-structuring raw documents. It is designed to be used with academic papers and it uses a cascade of sequence labelling models, that can be selected among ML architecture such as CRF and other DL models. The models do not work on text but on Layout Tokens. Grobid could be used (and integrated into Langchain) to parse papers in PDF format into Documents.</p> <p>The basic experiment (without RAG) is performed using this prompt:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System: The user will provide one or more questions 
related to a scientific argument. 
You are a paper writing assistant that 
writes a paragraph that answers to the questions.

Notice that the paragraph will be 
part of a scientific paper, 
so the writing style must be technical and formal.


Human: What are the main components and the
key innovations of Progressive Contextual Instance Refinement (PCIR)?

</code></pre></div></div> <p>The answer is inaccurate and partially hallucinating.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Progressive Contextual Instance Refinement (PCIR) is a 
novel approach in the field of computer vision that 
aims to improve the accuracy of object detection and 
instance segmentation tasks. 

The main components of PCIR include a backbone network, 
a contextual module, and an instance refinement module. 

The backbone network is responsible for extracting 
high-level features from the input image. 

The contextual module utilizes contextual information 
to enhance the representation of objects and their surrounding context. 

This module incorporates a contextual attention mechanism 
that selectively focuses on relevant regions. 

The instance refinement module refines the initial object 
proposals by iteratively updating the bounding box coordinates 
and segmentation masks. 

It employs a progressive refinement strategy that 
gradually improves the accuracy of the detected instances. 

The key innovation of PCIR lies in its ability to 
effectively leverage contextual information and refine 
object instances in a progressive manner, 
leading to improved performance in object detection and 
instance segmentation tasks.
</code></pre></div></div> <p>Applying RAG, the prompt became the following (with the context filled at run time):</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>System: The user will provide one or more questions 
related to a scientific argument. 
You are a paper writing assistant that 
writes a paragraph that answers to the questions.

Notice that the paragraph will be 
part of a scientific paper, 
so the writing style must be technical and formal.

Use the following pieces of retrieved context 
to write a more complete paragraph:
Context: {context}

Human: What are the main components and the
key innovations of Progressive Contextual Instance Refinement (PCIR)?
</code></pre></div></div> <p>In this case the result is much more precise and compatible with the writing style of a scientific paper.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The main components of Progressive Contextual Instance Refinement 
(PCIR) are the DCIR module and the PPSP module. 

The DCIR module is responsible for mining all 
potential instances existing in the images, 
while the PPSP module removes noisy negative samples 
and maintains the diversity of the samples. 

These two modules work together to progressively 
refine the contextual instances. 

The key innovation of PCIR lies in its ability 
to effectively combine these two modules to 
achieve superior performance in weakly supervised object detection. 

Experiments conducted on the NWPU VHR-10.v2 and 
DIOR data sets have clearly demonstrated the 
effectiveness of PCIR compared to state-of-the-art 
methods under a weakly supervised paradigm.
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/LLM_research_assistant.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Samuele Pasini. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>